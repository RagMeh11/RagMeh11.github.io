---
---
%%%%%% book

@book{sudre2024uncertainty,
  abbr={Book},
  bibtex_show={true},
  title={Uncertainty for Safe Utilization of Machine Learning in Medical Imaging: 6th International Workshop, UNSURE 2024, Held in Conjunction with MICCAI 2024, Marrakesh, Morocco, October 10, 2024, Proceedings},
  author={Sudre, Carole H and Mehta, Raghav and Ouyang, Cheng, and Qin, Chen and Rakic, Marianne and Wells, William M},
  volume={15167},
  month={October},
  year={2024},
  publisher={Springer Nature},
  doi={https://doi.org/10.1007/978-3-031-73158-7},
  html={https://link.springer.com/book/10.1007/978-3-031-73158-7}
}


@book{sudre2023uncertainty,
  abbr={Book},
  bibtex_show={true},
  title={Uncertainty for Safe Utilization of Machine Learning in Medical Imaging: 5th International Workshop, UNSURE 2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 12, 2023, Proceedings},
  author={Sudre, Carole H and Baumgartner, Christian F and Dalca, Adrian and Mehta, Raghav and Qin, Chen and Wells, William M},
  volume={14291},
  month={October},
  year={2023},
  publisher={Springer Nature},
  doi={https://doi.org/10.1007/978-3-031-44336-7},
  html={https://link.springer.com/book/10.1007/978-3-031-44336-7}
}

%%% Journals

@article{melba:2022:029:nichyporuk,
    abbr={MELBA},
    bibtex_show={true},
    title={Rethinking Generalization: The Impact of Annotation Style on Medical Image Segmentation},
    author={Nichyporuk, Brennan and Cardinell, Jillian and Szeto, Justin and Mehta, Raghav and Falet, Jean-Pierre and Arnold, Douglas L. and Tsaftaris, Sotirios A. and Arbel, Tal},
    abstract={Generalization is an important attribute of machine learning models, particularly for those that are to be deployed in a medical context, where unreliable predictions can have real world consequences. While the failure of models to generalize across datasets is typically attributed to a mismatch in the data distributions, performance gaps are often a consequence of biases in the "ground-truth" label annotations. This is particularly important in the context of medical image segmentation of pathological structures (e.g. lesions), where the annotation process is much more subjective, and affected by a number underlying factors, including the annotation protocol, rater education/experience, and clinical aims, among others. In this paper, we show that modeling annotation biases, rather than ignoring them, poses a promising way of accounting for differences in annotation style across datasets. To this end, we propose a generalized conditioning framework to (1) learn and account for different annotation styles across multiple datasets using a single model, (2) identify similar annotation styles across different datasets in order to permit their effective aggregation, and (3) fine-tune a fully trained model to a new annotation style with just a few samples. Next, we present an image-conditioning approach to model annotation styles that correlate with specific image features, potentially enabling detection biases to be more easily identified.},
    journal={Machine Learning for Biomedical Imaging (MELBA) Journal},
    volume={1},
    issue={December 2022 issue},
    month={December},
    year={2022},
    pages={1--37},
    issn={2766-905X},
    doi={https://doi.org/10.59275/j.melba.2022-2d93},
    html={https://melba-journal.org/2022:029},
    arxiv={2210.17398}
}

@article{melba:2022:026:mehta,
    abbr={MELBA},
    bibtex_show={true},
    title={QU-BraTS: MICCAI BraTS 2020 Challenge on Quantifying Uncertainty in Brain Tumor Segmentation – Analysis of Ranking Scores and Benchmarking Results},
    author={Mehta, Raghav and Filos, Angelos and Baid, Ujjwal and Sako, Chiharu and McKinley, Richard and Rebsamen, Michael and Dätwyler, Katrin and Meier, Raphael and Radojewski, Piotr and Murugesan, Gowtham Krishnan and Nalawade, Sahil and Ganesh, Chandan and Wagner, Ben and Yu, Fang F. and Fei, Baowei and Madhuranthakam, Ananth J. and Maldjian, Joseph A. and Daza, Laura and Gómez, Catalina and Arbeláez, Pablo and Dai, Chengliang and Wang, Shuo and Reynaud, Hadrien and Mo, Yuanhan and Angelini, Elsa and Guo, Yike and Bai, Wenjia and Banerjee, Subhashis and Pei, Linmin and AK, Murat and Rosas-González, Sarahi and Zemmoura, Ilyess and Tauber, Clovis and Vu, Minh H. and Nyholm, Tufve and Löfstedt, Tommy and Ballestar, Laura Mora and Vilaplana, Veronica and McHugh, Hugh and Maso Talou, Gonzalo and Wang, Alan and Patel, Jay and Chang, Ken and Hoebel, Katharina and Gidwani, Mishka and Arun, Nishanth and Gupta, Sharut and Aggarwal, Mehak and Singh, Praveer and Gerstner, Elizabeth R. and Kalpathy-Cramer, Jayashree and Boutry, Nicolas and Huard, Alexis and Vidyaratne, Lasitha and Rahman, Md Monibor and Iftekharuddin, Khan M. and Chazalon, Joseph and Puybareau, Elodie and Tochon, Guillaume and Ma, Jun and Cabezas, Mariano and Llado, Xavier and Oliver, Arnau and Valencia, Liliana and Valverde, Sergi and Amian, Mehdi and Soltaninejad, Mohammadreza and Myronenko, Andriy and Hatamizadeh, Ali and Feng, Xue and Dou, Quan and Tustison, Nicholas and Meyer, Craig and Shah, Nisarg A. and Talbar, Sanjay and Weber, Marc-André and Mahajan, Abhishek and Jakab, Andras and Wiest, Roland and Fathallah-Shaykh, Hassan M. and Nazeri, Arash and Milchenko, Mikhail and Marcus, Daniel and Kotrotsou, Aikaterini and Colen, Rivka and Freymann, John and Kirby, Justin and Davatzikos, Christos and Menze, Bjoern and Bakas, Spyridon and Gal, Yarin and Arbel, Tal},
    abstract={Deep learning (DL) models have provided the state-of-the-art performance in a wide variety of medical imaging benchmarking challenges, including the Brain Tumor Segmentation (BraTS) challenges. However, the task of focal pathology multi-compartment segmentation (e.g., tumor and lesion sub-regions) is particularly challenging, and potential errors hinder the translation of DL models into clinical workflows. Quantifying the reliability of DL model predictions in the form of uncertainties, could enable clinical review of the most uncertain regions, thereby building trust and paving the way towards clinical translation. Recently, a number of uncertainty estimation methods have been introduced for DL medical image segmentation tasks. Developing scores to evaluate and compare the performance of uncertainty measures will assist the end-user in making more informed decisions. In this study, we explore and evaluate a score developed during the BraTS 2019-2020 task on uncertainty quantification (QU-BraTS), and designed to assess and rank uncertainty estimates for brain tumor multi-compartment segmentation. This score (1) rewards uncertainty estimates that produce high confidence in correct assertions, and those that assign low confidence levels at incorrect assertions, and (2) penalizes uncertainty measures that lead to a higher percentages of under-confident correct assertions. We further benchmark the segmentation uncertainties generated by 14 independent participating teams of QU-BraTS 2020, all of which also participated in the main BraTS segmentation task. Overall, our findings confirm the importance and complementary value that uncertainty estimates provide to segmentation algorithms, and hence highlight the need for uncertainty quantification in medical image analyses. },
    journal={Machine Learning for Biomedical Imaging (MELBA) Journal},
    volume={1},
    issue={August 2022 issue},
    year={2022},
    month={August},
    pages={1--54},
    issn={2766-905X},
    doi={https://doi.org/10.59275/j.melba.2022-354b},
    html={https://melba-journal.org/2022:026},
    arxiv={2112.10074},
    selected={true}
}

@article{mehta2021propagating,
  abbr={TMI},
  bibtex_show={true},
  title={Propagating uncertainty across cascaded medical imaging tasks for improved deep learning inference},
  author={Mehta, Raghav and Christinck, Thomas and Nair, Tanya and Bussy, Aur{\'e}lie and Premasiri, Swapna and Costantino, Manuela and Chakravarthy, M Mallar and Arnold, Douglas L and Gal, Yarin and Arbel, Tal},
  journal={IEEE Transactions on Medical Imaging (TMI)},
  abstract={Although deep networks have been shown to perform very well on a variety of medical imaging tasks, inference in the presence of pathology presents several challenges to common models. These challenges impede the integration of deep learning models into real clinical workflows, where the customary process of cascading deterministic outputs from a sequence of image-based inference steps (e.g. registration, segmentation) generally leads to an accumulation of errors that impacts the accuracy of downstream inference tasks. In this paper, we propose that by embedding uncertainty estimates across cascaded inference tasks, performance on the downstream inference tasks should be improved. We demonstrate the effectiveness of the proposed approach in three different clinical contexts: (i) We demonstrate that by propagating T2 weighted lesion segmentation results and their associated uncertainties, subsequent T2 lesion detection performance is improved when evaluated on a proprietary large-scale, multi-site, clinical trial dataset acquired from patients with Multiple Sclerosis. (ii) We show an improvement in brain tumour segmentation performance when the uncertainty map associated with a synthesised missing MR volume is provided as an additional input to a follow-up brain tumour segmentation network, when evaluated on the publicly available BraTS-2018 dataset. (iii) We show that by propagating uncertainties from a voxel-level hippocampus segmentation task, the subsequent regression of the Alzheimer’s disease clinical score is improved.},
  volume={41},
  number={2},
  pages={360--373},
  year={2021},
  month={October},
  publisher={IEEE},
  doi={https://doi.org/10.1109/TMI.2021.3114097},
  html={https://ieeexplore.ieee.org/document/9541203},
  selected={true}
}

@article{sivaswamy2019construction,
  abbr={NI},
  bibtex_show={true},
  title={Construction of Indian human brain atlas},
  author={Sivaswamy, Jayanthi and Thottupattu, Alphin J and Mehta, Raghav and Sheelakumari, R and Kesavadas, Chandrasekharan},
  abstract={A brain MRI atlas plays an important role in many neuroimage analysis tasks as it provides an atlas with a standard co-ordinate system which is needed for spatial normalization of a brain MRI. Ideally, this atlas should be as near to the average brain of the population being studied as possible. Hence, correction for age and gender is typically done by selecting age- and gender-appropriate atlases. The MNI152 \cite{MNI152} is used as a standard atlas in many studies. MNI152 is constructed using T1 brain MRI scan of 152 Caucasian subjects. Similarly, the LPBA40 atlas derived from 40 ethnically diverse subjects is popular in segmentation as it provides structure probabilty maps \cite{lonii} for 56 cortical structures of 40 brain volumes. However, there is emerging evidence for morphological difference across populations especially in terms of global brain features like height, width and length which suggest that population-specific atlases may also be needed for accurate analysis. We report on the construction of a brain atlas of subjects from India. In the first part of this paper, we construct and validate the Indian brain MRI atlas of young Indian population and the corresponding structure probability maps. Next we also report our findings based on comparison of the Indian brain atlas with other population-specific atlases. The findings confirm that there is significant morphological difference between Indian, Chinese and Caucasian populations.},
  journal={Neurology India (NI) Journal},
  volume={67},
  number={1},
  pages={229},
  year={2019},
  month={January},
  publisher={Medknow Publications},
  doi={https://doi.org/10.4103/0028-3886.253639},
  html={https://pubmed.ncbi.nlm.nih.gov/30860125/},
  selected={true}
}


@article{mehta2017brainsegnet,
  abbr={JMI},
  bibtex_show={true},
  title={BrainSegNet: a convolutional neural network architecture for automated segmentation of human brain structures},
  author={Mehta, Raghav and Majumdar, Aabhas and Sivaswamy, Jayanthi},
  abstract={Automated segmentation of cortical and noncortical human brain structures has been hitherto approached using nonrigid registration followed by label fusion. We propose an alternative approach for this using a convolutional neural network (CNN) which classifies a voxel into one of many structures. Four different kinds of two-dimensional and three-dimensional intensity patches are extracted for each voxel, providing local and global (context) information to the CNN. The proposed approach is evaluated on five different publicly available datasets which differ in the number of labels per volume. The obtained mean Dice coefficient varied according to the number of labels, for example, it is 0.844+-0.031 and 0.743+-0.019 for datasets with the least (32) and the most (134) number of labels, respectively. These figures are marginally better or on par with those obtained with the current state-of-the-art methods on nearly all datasets, at a reduced computational time. The consistently good performance of the proposed method across datasets and no requirement for registration make it attractive for many applications where reduced computational time is necessary.},
  journal={Journal of Medical Imaging (JMI)},
  volume={4},
  number={2},
  pages={024003--024003},
  year={2017},
  month={April},
  publisher={Society of Photo-Optical Instrumentation Engineers (SPIE)},
  doi={https://doi.org/10.1117/1.JMI.4.2.024003},
  html={https://www.spiedigitallibrary.org/journals/Journal-of-Medical-Imaging/volume-4/issue-2/024003/BrainSegNet--a-convolutional-neural-network-architecture-for-automated-segmentation/10.1117/1.JMI.4.2.024003.short?SSO=1#_=_}
}


%%%% conferences

@inproceedings{ribeiro2025flowssn,
  abbr={ICCV},
  bibtex_show={true},
  title={Flow Stochastic Segmentation Networks},
  author={De Sousa Ribeiro, Fabio and Todd, Omar and Jones, Charles and Kori, Avinash and Mehta, Raghav and Glocker, Ben},
  booktitle={International Conference on Computer Vision (ICCV)},
  abstract={We propose the Flow Stochastic Segmentation Network (Flow-SSN), a generative model for probabilistic segmentation featuring discrete-time autoregressive and modern continuous-time flow parameterisations. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, as most of the model capacity is allocated to learning the base distribution of the flow, which constitutes an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results.},
  pages={},
  month={October},
  doi={},
  html={},
  arxiv={2507.18838},
  year={2025},
}


@inproceedings{gopinath2025unsurf,
  abbr={MICCAI},
  bibtex_show={true},
  title={UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction of Clinical Brain MRIs},
  author={Mehta, Raghav and Gopinath, Karthik and Glocker, Ben and Eugenio Iglesias, Juan},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={We propose UNSURF, a novel uncertainty measure for cortical surface reconstruction of clinical brain MRI scans of any orientation, resolution, and contrast. It relies on the discrepancy between predicted voxel-wise signed distance functions (SDFs) and the actual SDFs of the fitted surfaces. Our experiments on real clinical scans show that traditional uncertainty measures, such as voxel-wise Monte Carlo variance, are not suitable for modeling the uncertainty of surface placement. Our results demonstrate that UNSURF estimates correlate well with the ground truth errors and: (i) enable effective automated quality control of surface reconstructions at the subject-, parcel-, mesh node-level; and (ii) improve performance on a downstream Alzheimer’s disease classification task.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  doi={},
  html={},
  arxiv={2506.00498},
  selected={true}
}


@inproceedings{mehta2025cfseg,
  abbr={MICCAI},
  bibtex_show={true},
  title={CF-Seg: Counterfactuals Meet Segmentation},
  author={Mehta, Raghav and De Sousa Ribeiro, Fabio and Xia, Tian and Roschewitz, Melanie and Santhirasekaram, Ainkaran and Marshall, Dominic and Glocker, Ben},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={Segmenting anatomical structures in medical images plays an important role in the quantitative assessment of various diseases. However, accurate segmentation becomes significantly more challenging in the presence of disease. Disease patterns can alter the appearance of surrounding healthy tissues, introduce ambiguous boundaries, or even obscure critical anatomical structures. As such, segmentation models trained on real-world datasets may struggle to provide good anatomical segmentation, leading to potential misdiagnosis. In this paper, we generate counterfactual (CF) images to simulate how the same anatomy would appear in the absence of disease without altering the underlying structure. We then use these CF images to segment structures of interest, without requiring any changes to the underlying segmentation model. Our experiments on two real-world clinical chest X-ray datasets show that the use of counterfactual images improves anatomical segmentation, thereby aiding downstream clinical decision-making.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  doi={},
  html={},
  arxiv={2506.16213},
  selected={true},
}

@inproceedings{xia2025segcft,
  abbr={MICCAI},
  bibtex_show={true},
  title={Segmentor-guided Counterfactual Fine-Tuning for Locally Coherent and Targeted Image Synthesis},
  author={Xia, Tian and Sinclair, Matthew and Schuh, Andreas and De Sousa Ribeiro, Fabio and Mehta, Raghav and Rasal, Rajat and Puyol-Enton, Esther and Gerber, Samuel and Petersen, Kersten and Schaap, Michael and Glocker, Ben},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT) which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code will be made publicly available.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  doi={},
  html={},
  arxiv={},
}

@inproceedings{roschewitz2025shiftidenti,
  abbr={MICCAI},
  bibtex_show={true},
  title={Automatic dataset shift identification to support safe deployment of medical imaging AI},
  author={Roschewitz, Melanie and Mehta, Raghav and Jones, Charles and Glocker, Ben},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and the choice of shift mitigation strategies highly depends on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework, effectively distinguishing between prevalence shift, covariate shift and mixed shifts. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts, using five large publicly available datasets.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  doi={},
  html={},
  arxiv={2411.07940},
}

@inproceedings{shui2023mitigating,
  abbr={MICCAI},
  bibtex_show={true},
  title={Mitigating calibration bias without fixed attribute grouping for improved fairness in medical imaging analysis},
  author={Shui, Changjian and Szeto, Justin and Mehta, Raghav and Arnold, Douglas L and Arbel, Tal},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis. Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.},
  pages={189--198},
  year={2023},
  month={October},
  organization={Springer},
  doi={https://doi.org/10.1007/978-3-031-43898-1_19},
  html={https://link.springer.com/chapter/10.1007/978-3-031-43898-1_19},
  arxiv={2307.01738},
  selected={true},
  hnote={Early Acceptance}
}


@inproceedings{durso2023improving,
  abbr={MICCAI},
  bibtex_show={true},
  title={Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models},
  author={Durso-Finley, Joshua and Falet, Jean-Pierre and Mehta, Raghav and Arnold, Douglas L and Pawlowski, Nick and Arbel, Tal},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  abstract={Image-based precision medicine aims to personalize treatment decisions based on an individual’s unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of the uncertainty estimate with the factual error, and, given the lack of ground truth counterfactual outcomes, demonstrate how uncertainty for the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate how knowledge of uncertainty could modify clinical decision-making to improve individual patient and clinical trial outcomes.},
  pages={72--481},
  year={2023},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-031-43904-9_46},
  doi={https://doi.org/10.1007/978-3-031-43904-9_46},
  arxiv={2305.03829},
  selected={true},
  hnote={Student Travel Award (Top 10 paper)}
}


@inproceedings{mehta2023evaluating,
  abbr={MIDL},
  bibtex_show={true},
  title={Evaluating the Fairness of Deep Learning Uncertainty Estimates in Medical Image Analysis},
  author={Mehta, Raghav and Shui, Changjian and Arbel, Tal},
  abstract={Although deep learning (DL) models have shown great success in many medical image analysis tasks, deployment of the resulting models into real clinical contexts requires: (1) that they exhibit robustness and fairness across different sub-populations, and (2) that the confidence in DL model predictions be accurately expressed in the form of uncertainties. Unfortunately, recent studies have indeed shown significant biases in DL models across demographic subgroups (e.g., race, sex, age) in the context of medical image analysis, indicating a lack of fairness in the models. Although several methods have been proposed in the ML literature to mitigate a lack of fairness in DL models, they focus entirely on the absolute performance between groups without considering their effect on uncertainty estimation. In this work, we present the first exploration of the effect of popular fairness models on overcoming biases across subgroups in medical image analysis in terms of bottom-line performance, and their effects on uncertainty quantification. We perform extensive experiments on three different clinically relevant tasks: (i) skin lesion classification, (ii) brain tumour segmentation, and (iii) Alzheimer's disease clinical score regression. Our results indicate that popular ML methods, such as data-balancing and distributionally robust optimization, succeed in mitigating fairness issues in terms of the model performances for some of the tasks. However, this can come at the cost of poor uncertainty estimates associated with the model predictions. This tradeoff must be mitigated if fairness models are to be adopted in medical image analysis. },
  booktitle={Medical Imaging with Deep Learning (MIDL) conference},
  pages={000-000},
  year={2023},
  month={July},
  organization={PMLR},
  html={https://arxiv.org/abs/2303.03242},
  arxiv={2303.03242},
  poster={MIDL_2023_Fairness_Uncertainty.pdf},
  selected={true}
}


@inproceedings{vadacchino2021had,
  abbr={MIDL},
  bibtex_show={true},
  title={Had-net: A hierarchical adversarial knowledge distillation network for improved enhanced tumour segmentation without post-contrast images},
  author={Vadacchino, Saverio and Mehta, Raghav and Sepahvand, Nazanin Mohammadi and Nichyporuk, Brennan and Clark, James J and Arbel, Tal},
  abstract={Segmentation of enhancing tumours or lesions from MRI is important for detecting new disease activity in many clinical contexts. However, accurate segmentation requires the inclusion of medical images (e.g., T1 post-contrast MRI) acquired after injecting patients with a contrast agent (e.g., Gadolinium), a process no longer thought to be safe. Although a number of modality-agnostic segmentation networks have been developed over the past few years, they have been met with limited success in the context of enhancing pathology segmentation. In this work, we present HAD-Net, a novel offline adversarial knowledge distillation (KD) technique, whereby a pre-trained teacher segmentation network, with access to all MRI sequences, teaches a student network, via hierarchical adversarial training, to better overcome the large domain shift presented when crucial images are absent during inference. In particular, we apply HAD-Net to the challenging task of enhancing tumour segmentation when access to post-contrast imaging is not available. The proposed network is trained and tested on the BraTS 2019 brain tumour segmentation challenge dataset, where it achieves performance improvements in the ranges of 16% - 26% over (a) recent modality-agnostic segmentation methods (U-HeMIS, U-HVED), (b) KD-Net adapted to this problem, (c) the pre-trained student network and (d) a non-hierarchical version of the network (AD-Net), in terms of Dice scores for enhancing tumour (ET). The network also shows improvements in tumour core (TC) Dice scores. Finally, the network outperforms both the baseline student network and AD-Net in terms of uncertainty quantification for enhancing tumour segmentation based on the BraTS 2019 uncertainty challenge metrics. },
  booktitle={Medical Imaging with Deep Learning (MIDL) conference},
  pages={787--801},
  year={2021},
  month={July},
  organization={PMLR},
  html={https://proceedings.mlr.press/v143/vadacchino21a.html},
  arxiv={2103.16617},
  slides={HAD_Net_Presentation_MIDL_Short.pdf}
}

@article{mehta2020uncertainty,
  abbr={MIDL},
  bibtex_show={true},
  title={Uncertainty evaluation metric for brain tumour segmentation},
  author={Mehta, Raghav and Filos, Angelos and Gal, Yarin and Arbel, Tal},
  abstract={n this paper, we develop a metric designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to: (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions. Here, the workings of the components of the metric are explored based on a number of popular uncertainty measures evaluated on the BraTS 2019 dataset},
  journal={Medical Imaging with Deep Learning (MIDL) Short Papers},
  html={https://arxiv.org/abs/2005.14262},
  arxiv={2005.14262},
  year={2020},
  month={May},
  slides={Mehta20-slides.pdf}
}

@inproceedings{mehta2017m,
  abbr={ISBI},
  bibtex_show={true},
  title={M-net: A convolutional neural network for deep brain structure segmentation},
  author={Mehta, Raghav and Sivaswamy, Jayanthi},
  booktitle={2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI)},
  abstract={In this paper, we propose an end-to-end trainable Convolutional Neural Network (CNN) architecture called the M-net, for segmenting deep (human) brain structures from Magnetic Resonance Images (MRI). A novel scheme is used to learn to combine and represent 3D context information of a given slice in a 2D slice. Consequently, the M-net utilizes only 2D convolution though it operates on 3D data, which makes M-net memory efficient. The segmentation method is evaluated on two publicly available datasets and is compared against publicly available model based segmentation algorithms as well as other classification based algorithms such as Random Forrest and 2D CNN based approaches. Experiment results show that the M-net outperforms all these methods in terms of dice coefficient and is at least 3 times faster than other methods in segmenting a new volume which is attractive for clinical use.},
  pages={437--440},
  year={2017},
  month={April},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/7950555},
  doi={https://doi.org/10.1109/ISBI.2017.7950555},
  slides={ISBI_2017_Mnet.pdf},
  onote={Oral Presentation}
}

@inproceedings{mehta2016hybrid,
  abbr={ISBI},
  bibtex_show={true},
  title={A hybrid approach to tissue-based intensity standardization of brain MRI images},
  author={Mehta, Raghav and Sivaswamy, Jayanthi},
  abstract={The variations in the intensity scale in Magnetic Resonance Images pose a problem for many tasks and Intensity Standardization (IS) aims to solve this problem. Existing methods generally use landmark values of the image histogram and match it to a standard scale. The landmarks are often chosen to be percentiles from different segmented tissues. We propose a method for IS in which tissue information (via segmentation) is needed during training but not during testing by using landmark propagation. A KL divergence-based technique is employed for identifying volumes from the training set, which are similar to a given non-standardized testing volume. The landmarks from the similar volumes are then propagated to the given test volume. Evaluation of the proposed method on 24 MRI volumes from 3 different scanners shows that the IS results are better than L4 and at par with a method which uses prior segmentation, to get percentile-based landmarks. The proposed method aids speeding up and expanding the scope of IS to volumes with no tissue information.},
  booktitle={2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)},
  pages={95--98},
  year={2016},
  month={April},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/7493219},
  doi={https://doi.org/10.1109/ISBI.2016.7493219},
  poster={ISBI_IS_Poster_Draft_print.pdf}
}


%%%%% workshops

@inproceedings{todd2025delineation,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Delineation Uncertainty from Clinician Ranges in Cervical Cancer Radiotherapy Planning},
  author={Todd, Omar and Kim, Sooha and Mackay, Katherine and Mehta, Raghav, De Sousa Ribeiro, Fabio and Bernstein, David and Taylor, Alexandra and Glocker, Ben},
  booktitle={MICCAI Workshop on Statistical Atlases and Computational Modeling of the Heart (STACOM)},
  abstract={Accurate contouring of target areas affected by cancer is crucial in radiotherapy planning to provide effective treatment for patients. Beyond producing sufficiently precise contours, it is also important to have a reliable measure of the associated uncertainty of where the true anatomical boundaries may be. Inter-observer variability arising from multiple experts annotating the boundaries of the target area provides a representation of this uncertainty; however, these annotations are labour-intensive to obtain over a large number of images, often making it infeasible in practice. In this study, we evaluate the clinical relevance of predictive auto-contouring uncertainty from training on clinician-defined ranges -- a set of annotations from a single expert representing the uncertainty of the target area, which are far more efficient to produce. This aims to bridge the gap in the performance of uncertainty estimates between training on a single contour and a set of contours from multiple annotators. This is achieved through the curation of a cervical cancer dataset with uncertainty annotations produced on CT scans from radiotherapy patients. We demonstrate that the resulting uncertainty from a model trained on these clinician-defined ranges is more meaningful compared with training on single contours without an uncertainty range. We also validated it as being clinically useful with respect to the inter-observer variation on a small hold-out set.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  html={},
  doi={},
  arxiv={},
}


@inproceedings{mittal2025cvd,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Cardiovascular disease classification using radiomics and geometric features from cardiac CT},
  author={Mittal, Ajay and Mehta, Raghav and Todd, Omar and Seeböck, Philipp and Langs, Georg and Glocker, Ben},
  booktitle={MICCAI Workshop on Statistical Atlases and Computational Modeling of the Heart (STACOM)},
  abstract={Automatic detection and classification of Cardiovascular disease (CVD) from Computed Tomography (CT) images play an important part in facilitating better-informed clinical decisions. However, most of the recent deep learning based methods either directly work on raw CT data or utilize it in pair with anatomical cardiac structure segmentation by training an end-to-end classifier. As such, these approaches become much more difficult to interpret from a clinical perspective. To address this challenge, in this work, we break down the CVD classification pipeline into three components: (i) image segmentation, (ii) image registration, and (iii) downstream CVD classification. Specifically, we utilize the Atlas-ISTN framework and recent segmentation foundational models to generate anatomical structure segmentation and a normative healthy atlas. These are further utilized to extract clinically interpretable radiomic features as well as deformation field based geometric features (through atlas registration) for CVD classification. Our experiments on the publicly available ASOCA dataset show that utilizing these features leads to better CVD classification accuracy (87.50%) when compared against classification model trained directly on raw CT images (67.50%).},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  html={},
  doi={},
  arxiv={2506.22226},
}


@inproceedings{stanley2025exploring,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification},
  author={Stanley, Emma and Mehta, Raghav and Roschewitz, Melanie and Forkert, Nils and Glocker, Ben},
  booktitle={MICCAI Workshop on Fairness of AI in Medical Imaging (FAIMI)},
  abstract={Systematic mislabelling affecting specific subgroups (\textit{i.e.,} label bias) in medical imaging datasets represents an understudied issue concerning the fairness of medical AI systems. In this work, we investigated how size and separability of subgroups affected by label bias influence the learned features and performance of a deep learning model. Therefore, we trained deep learning models for binary tissue density classification using the EMory BrEast imaging Dataset (EMBED), where label bias affected separable subgroups (based on imaging manufacturer) or non-separable `pseudo-subgroups'. We found that simulated subgroup label bias led to prominent shifts in the learned feature representations of the models. Importantly, these shifts within the feature space were dependent on both the relative size and the separability of the subgroup affected by label bias. We also observed notable differences in subgroup performance depending on whether a validation set with clean labels was used to define the classification threshold for the model. For instance, with label bias affecting the majority separable subgroup, the true positive rate for that subgroup fell from 0.898, when the validation set had clean labels, to 0.518, when the validation set had biased labels. Our work represents a key contribution toward understanding the consequences of label bias on subgroup fairness in medical imaging AI.},
  pages={},
  year={2025},
  month={October},
  organization={Springer},
  html={},
  doi={},
  arxiv={2507.17996}
}


@inproceedings{kumar2023debiasing,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Debiasing Counterfactuals in the Presence of Spurious Correlations},
  author={Kumar, Amar and Fathi, Nima and Mehta, Raghav and Nichyporuk, Brennan and Falet, Jean-Pierre R and Tsaftaris, Sotirios and Arbel, Tal},
  booktitle={MICCAI Workshop on Fairness of AI in Medical Imaging (FAIMI)},
  abstract={Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.},
  pages={276--286},
  year={2023},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-031-45249-9_27},
  doi={https://doi.org/10.1007/978-3-031-45249-9_27},
  slides={FAIMI_MICCAI2023_Amar.pdf},
  poster={FAIMI_2023_Poster.pdf},
  arxiv={2308.10984},
  hnote={Best Oral Presentation Award},
  onote={Oral Presentation}
}

@inproceedings{albiero2023confusing,
    abbr={ICCV-w},
    bibtex_show={true},
    author={Albiero, V{\'\i}tor and Mehta, Raghav and Evtimov, Ivan and Bell, Samuel and Sagun, Levent and Markosyan, Aram},
    title={Confusing Large Models by Confusing Small Models},
    abstract={Despite a steady growth in average accuracy, computer vision models continue to fail on many robustness benchmarks. In this paper, we take a step back from standard benchmarks and focus on how models perceive data, and which aspects of the data they find confusing. Using an ensemble-based confusion score built on top of simple calibrations we examine how the training and test samples appear simple or confusing to a given model. Based on these heuristics, we demonstrate an application of the confusion score in identifying images that appear confusing to the trained model, and show that these images are highly likely to be misclassified by the model. We further demonstrate how confusion carries over to models of various sizes and architectures, which gives rise to the possibility of identifying challenging images via ensembles of small networks to produce a custom benchmark of challenging data, that remains appropriate for large models where ensembling is costly to implement. Finally, we demonstrate how training via upsampling on confusing images can improve accuracy on the hard subset.},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    year={2023},
    month={October},
    pages={4304-4312},
    html={https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Albiero_Confusing_Large_Models_by_Confusing_Small_Models_ICCVW_2023_paper.html},
    onote={Oral Presentation}
}


@inproceedings{mehta2022you,
    abbr={ECCV-w},
    bibtex_show={true},
    title={You only need a good embeddings extractor to fix spurious correlations},
    author={Mehta, Raghav and Albiero, V{\'\i}tor and Chen, Li and Evtimov, Ivan and Glaser, Tamar and Li, Zhiheng and Hassner, Tal},
    abstract={Spurious correlations in training data often lead to robustness issues since models learn to use them as shortcuts. For example, when predicting whether an object is a cow, a model might learn to rely on its green background, so it would do poorly on a cow on a sandy background. A standard dataset for measuring state-of-the-art on methods mitigating this problem is Waterbirds. The best method (Group Distributionally Robust Optimization - GroupDRO) currently achieves 89\% worst group accuracy and standard training from scratch on raw images only gets 72\%. GroupDRO requires training a model in an end-to-end manner with subgroup labels. In this paper, we show that we can achieve up to 90\% accuracy without using any sub-group information in the training set by simply using embeddings from a large pre-trained vision model extractor and training a linear classifier on top of it. With experiments on a wide range of pre-trained models and pre-training datasets, we show that the capacity of the pre-training model and the size of the pre-training dataset matters. Our experiments reveal that high capacity vision transformers perform better compared to high capacity convolutional neural networks, and larger pre-training dataset leads to better worst-group accuracy on the spurious correlation dataset.},    
    booktitle= {Proceedings of the IEEE/CVF European Conference on Computer Vision (ECCV) Workshops},
    year={2022},
    month={October},
    html={https://arxiv.org/abs/2212.06254},
    arxiv={2212.06254},
    slides={RCV_ECCV_2022_slides.pdf},
    onote={Oral Presentation}
}


@inproceedings{mehta2022information,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Information gain sampling for active learning in medical image classification},
  author={Mehta, Raghav and Shui, Changjian and Nichyporuk, Brennan and Arbel, Tal},
  booktitle={International Workshop on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging (UNSURE)},
  abstract={Large, annotated datasets are not widely available in medical image analysis due to the prohibitive time, costs, and challenges associated with labelling large datasets. Unlabelled datasets are easier to obtain, and in many contexts, it would be feasible for an expert to provide labels for a small subset of images. This work presents an information-theoretic active learning framework that guides the optimal selection of images from the unlabelled pool to be labeled based on maximizing the expected information gain (EIG) on an evaluation dataset. Experiments are performed on two different medical image classification datasets: multi-class diabetic retinopathy disease scale classification and multi-class skin lesion classification. Results indicate that by adapting EIG to account for class-imbalances, our proposed Adapted Expected Information Gain (AEIG) outperforms several popular baselines including the diversity based CoreSet and uncertainty based maximum entropy sampling. Specifically, AEIG achieves 95% of overall performance with only 19% of the training data, while other active learning approaches require around 25%. We show that, by careful design choices, our model can be integrated into existing deep learning classifiers.},
  pages={135--145},
  year={2022},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-031-16749-2_13},
  arxiv={2208.00974},
  doi={https://doi.org/10.1007/978-3-031-16749-2_13},
  poster={UNSURE_poster_2022.pdf},
  slides={UNSURE_2022_spotlight.pdf}
}


@inproceedings{nichyporuk2021cohort,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Cohort bias adaptation in aggregated datasets for lesion segmentation},
  author={Nichyporuk, Brennan and Cardinell, Jillian and Szeto, Justin and Mehta, Raghav and Tsaftaris, Sotirios and Arnold, Douglas L and Arbel, Tal},
  booktitle={International Workshop on Domain Adaptation and Representation Transfer (DART)},
  abstract={Many automatic machine learning models developed for focal pathology (e.g. lesions, tumours) detection and segmentation perform well, but do not generalize as well to new patient cohorts, impeding their widespread adoption into real clinical contexts. One strategy to create a more diverse, generalizable training set is to naively pool datasets from different cohorts. Surprisingly, training on this big data does not necessarily increase, and may even reduce, overall performance and model generalizability, due to the existence of cohort biases that affect label distributions. In this paper, we propose a generalized affine conditioning framework to learn and account for cohort biases across multi-source datasets, which we call Source-Conditioned Instance Normalization (SCIN). Through extensive experimentation on three different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS) clinical trial MRI datasets, we show that our cohort bias adaptation method (1) improves performance of the network on pooled datasets relative to naively pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the instance normalization parameters, thus learning the new cohort bias with only 10 labelled samples.},
  pages={101--111},
  year={2021},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-87722-4_10},
  arxiv={2108.00713},
  doi={https://doi.org/10.1007/978-3-030-87722-4_10},
  slides={DART_2021.pdf},
  onote={Oral Presentation},
  hnote={Best Paper Award}
}


@inproceedings{mehta2019propagating,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Propagating uncertainty across cascaded medical imaging tasks for improved deep learning inference},
  author={Mehta, Raghav and Christinck, Thomas and Nair, Tanya and Lemaitre, Paul and Arnold, Douglas and Arbel, Tal},
  abstract={Although deep networks have been shown to perform very well on a variety of tasks, inference in the presence of pathology in medical images presents challenges to traditional networks. Given that medical image analysis typically requires a sequence of inference tasks to be performed (e.g. registration, segmentation), this results in an accumulation of errors over the sequence of deterministic outputs. In this paper, we explore the premise that, by embedding uncertainty estimates across cascaded inference tasks, the final prediction results should improve over simply cascading the deterministic classification results or performing inference in a single stage. Specifically, we develop a deep learning framework that propagates voxel-based uncertainty measures (e.g. Monte Carlo (MC) dropout sample variance) across inference tasks in order to improve the detection and segmentation of focal pathologies (e.g. lesions, tumours) in brain MR images. We apply the framework to two different contexts. First, we demonstrate that propagating multiple sclerosis T2 lesion segmentation results along with their associated uncertainty measures improves subsequent T2 lesion detection accuracy when evaluated on a proprietary large-scale, multi-site, clinical trial dataset. Second, we show how by propagating uncertainties associated with a regressed 3D MRI volume as an additional input to a follow-on brain tumour segmentation task, one can improve segmentation results on the publicly available BraTS-2018 dataset.},
  booktitle={International Workshop on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging (UNSURE))},
  pages={23--32},
  year={2019},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-32689-0_3},
  doi={https://doi.org/10.1007/978-3-030-32689-0_3},
  slides={UNSURE_2019.pdf},
  poster={UNSURE_poster_final.pdf},
  onote={Oral Presentation},
  hnote={Best Paper Award}
}


@inproceedings{kaur2019improving,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={Improving pathological structure segmentation via transfer learning across diseases},
  author={Kaur, Barleen and Lema{\^\i}tre, Paul and Mehta, Raghav and Sepahvand, Nazanin Mohammadi and Precup, Doina and Arnold, Douglas and Arbel, Tal},
  abstract={One of the biggest challenges in developing robust machine learning techniques for medical image analysis is the lack of access to large-scale annotated image datasets needed for supervised learning. When the task is to segment pathological structures (e.g. lesions, tumors) from patient images, training on a dataset with few samples is very challenging due to the large class imbalance and inter-subject variability. In this paper, we explore how to best leverage a segmentation model that has been pre-trained on a large dataset of patients images with one disease in order to successfully train a deep learning pathology segmentation model for a different disease, for which only a relatively small patient dataset is available. Specifically, we train a UNet model on a large-scale, proprietary, multi-center, multi-scanner Multiple Sclerosis (MS) clinical trial dataset containing over 3500 multi-modal MRI samples with expert-derived lesion labels. We explore several transfer learning approaches to leverage the learned MS model for the task of multi-class brain tumor segmentation on the BraTS 2018 dataset. Our results indicate that adapting and fine-tuning the encoder and decoder of the network trained on the larger MS dataset leads to improvement in brain tumor segmentation when few instances are available. This type of transfer learning outperforms training and testing the network on the BraTS dataset from scratch as well as several other transfer learning approaches, particularly when only a small subset of the dataset is available.},
  booktitle={International Workshop on Domain Adaptation and Representation Transfer (DART)},
  pages={90--98},
  year={2019},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-33391-1_11},
  doi={https://doi.org/10.1007/978-3-030-33391-1_11},
  slides={DART_MICCAI.pdf},
  poster={DART_poster.pdf}
}


@inproceedings{mehta2018rs,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={RS-Net: Regression-segmentation 3D CNN for synthesis of full resolution missing brain MRI in the presence of tumours},
  author={Mehta, Raghav and Arbel, Tal},
  abstract={Accurate synthesis of a full 3D MR image containing tumours from available MRI (e.g. to replace an image that is currently unavailable or corrupted) would provide a clinician as well as downstream inference methods with important complementary information for disease analysis. In this paper, we present an end-to-end 3D convolution neural network that takes a set of acquired MR image sequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression of the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the tumour into subtypes (e.g. enhancement, core). The hypothesis is that this would focus the network to perform accurate synthesis in the area of the tumour. Experiments on the BraTS 2015 and 2017 datasets show that: (1) the proposed method gives better performance than state-of-the art methods in terms of established global evaluation metrics (e.g. PSNR), (2) replacing real MR volumes with the synthesized MRI does not lead to significant degradation in tumour and sub-structure segmentation accuracy. The system further provides uncertainty estimates based on Monte Carlo (MC) dropout for the synthesized volume at each voxel, permitting quantification of the system’s confidence in the output at each location.},
  booktitle={Third International Workshop on Simulation and Synthesis in Medical Imaging (SASHIMI)},
  pages={119--129},
  year={2018},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-00536-8_13},
  doi={https://doi.org/10.1007/978-3-030-00536-8_13},
  slides={SASHIMI-2018-presentation.pdf},
  arxiv={1807.10972},
  extendedreport={RSNet_extension.pdf},
  onote={Oral Presentation}
}

@inproceedings{majumdar2018learn,
  abbr={MICCAI-w},
  bibtex_show={true},
  title={To learn or not to learn features for deformable registration?},
  author={Majumdar, Aabhas and Mehta, Raghav and Sivaswamy, Jayanthi},
  abstract={Feature-based registration has been popular with a variety of features ranging from voxel intensity to Self-Similarity Context (SSC). In this paper, we examine the question of how features learnt using various Deep Learning (DL) frameworks can be used for deformable registration and whether this feature learning is necessary or not. We investigate the use of features learned by different DL methods in the current state-of-the-art discrete registration framework and analyze its performance on 2 publicly available datasets. We draw insights about the type of DL framework useful for feature learning. We consider the impact, if any, of the complexity of different DL models and brain parcellation methods on the performance of discrete registration. Our results indicate that the registration performance with DL features and SSC are comparable and stable across datasets whereas this does not hold for low level features. This shows that when handcrafted features are designed based on good insights into the problem at hand, they perform better or are comparable to features learnt using deep learning framework.},
  booktitle={First International Workshop on Deep Learning Fails (DLF)},
  pages={52--60},
  year={2018},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-02628-8_6},
  doi={https://doi.org/10.1007/978-3-030-02628-8_6},
  arxiv={1709.01057},
  slides={DLF-2018_ToLearnOrNot.pdf},
  onote={Oral Presentation}
}

@inproceedings{mehta20193d,
  abbr={MICCAI-c},
  bibtex_show={true},
  title={3D U-Net for brain tumour segmentation},
  abstract={In this work, we present a 3D Convolutional Neural Network (CNN) for brain tumour segmentation from Multimodal brain MR volumes. The network is a modified version of the popular 3D U-net architecture, which takes as input multi-modal brain MR volumes, processes them at multiple scales, and generates a full resolution multi-class tumour segmentation as output. The network is modified such that there is a better gradient flow in the network, which in turn should allow the network to learn better segmentation. The network is trained end-to-end on BraTS [1,2,3,4,5] 2018 Training dataset using a weighted Categorical Cross Entropy (CCE) loss function. A curriculum on class weights is employed to address the class imbalance issue. We achieve competitive segmentation results on BraTS 2018 Testing dataset with Dice scores of 0.706, 0.871, and 0.771 for enhancing tumour, whole tumour, and tumour core, respectively.},
  author={Mehta, Raghav and Arbel, Tal},
  booktitle={4th International Workshop on Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries (BrainLes)},
  pages={254--266},
  year={2018},
  month={October},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-030-11726-9_23},
  doi={https://doi.org/10.1007/978-3-030-11726-9_23},
  poster={BraTS_2018_poster_v2.pdf}
}

@inproceedings{kriz2023exploring,
  abbr={MICCAI-c},
  bibtex_show={true},
  title={Exploring Compound Loss Functions for Brain Tumor Segmentation},
  abstract={In this study, we introduce a modified 3D U-Net framework tailored for the BraTS 2023 Segmentation - Adult Glioma challenge. Alongside conventional techniques such as data augmentation, post-processing, and Monte Carlo dropout, we investigate the efficacy of compound loss functions with a primary focus on mitigating class imbalance. In particular, we investigate various combinations of cross-entropy, boundary, and dice loss functions to identify the most suitable loss for the given data distribution. By engineering the baseline U-Net model with these modifications, we have determined that the combination of dice and cross-entropy loss yields encouraging results, exemplified by lesion-wise dice scores of 0.753, 0.791, and 0.886. Our analysis justifies the use of specially designed loss functions for the underlying data distribution at hand.},
  author={Kriz, Anita and Mehta, Raghav and Nichyporuk, Brennan and Arbel, Tal},
  booktitle={International Challenge on Cross-Modality Domain Adaptation for Medical Image Segmentation},
  pages={300--311},
  year={2023},
  publisher={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-031-76163-8_27},
  doi={https://doi.org/10.1007/978-3-031-76163-8_27},
}

@techreport{sivaswamy2021sub,
  abbr={PrePrint},
  bibtex_show={true},
  abstract={Segmentation of sub-cortical structures from MRI scans is of interest in many neurological diagnosis. Since this is a laborious task machine learning and specifically deep learning (DL) methods have become explored. The structural complexity of the brain demands a large, high quality segmentation dataset to develop good DL-based solutions for sub-cortical structure segmentation. Towards this, we are releasing a set of 114, 1.5 Tesla, T1 MRI scans with manual delineations for 14 sub-cortical structures. The scans in the dataset were acquired from healthy young (21-30 years) subjects ( 58 male and 56 female) and all the structures are manually delineated by experienced radiology experts. Segmentation experiments have been conducted with this dataset and results demonstrate that accurate results can be obtained with deep-learning methods. Our sub-cortical structure segmentation dataset, Indian Brain Segmentation Dataset (IBSD) is made openly available.},
  title={Sub-cortical structure segmentation database for young population},
  author={Sivaswamy, Jayanthi and Thottupattu, Alphin J and Mehta, Raghav and Sheelakumari, R and Kesavadas, Chandrasekharan and others},
  journal={arXiv preprint},
  arxiv={2111.01561},
  year={2021},
  month={November}
}

@techreport{bakas2018identifying,
  abbr={PrePrint},
  bibtex_show={true},
  title={Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the {BRATS} challenge},
  author={Bakas, Spyridon and Reyes, Mauricio and Jakab, Andras and Bauer, Stefan and Rempfler, Markus and Crimi, Alessandro and Shinohara, Russell Takeshi and Berger, Christoph and Ha, Sung Min and Rozycki, Martin and Prastawa, Marcel and Alberts, Esther and Lipkova, Jana and Freymann, John and Kirby, Justin and Bilello, Michel and Fathallah-Shaykh, Hassan and Wiest, Roland and Kirschke, Jan and Wiestler, Benedikt and Colen, Rivka and Kotrotsou, Aikaterini and Lamontagne, Pamela and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Weber, Marc-Andre and Mahajan, Abhishek and Baid, Ujjwal and Gerstner, Elizabeth and Kwon, Dongjin and Acharya, Gagan and Agarwal, Manu and Alam, Mahbubul and Albiol, Alberto and Albiol, Antonio and Albiol, Francisco J and Alex, Varghese and Allinson, Nigel and Amorim, Pedro H A and Amrutkar, Abhijit and Anand, Ganesh and Andermatt, Simon and Arbel, Tal and Arbelaez, Pablo and Avery, Aaron and Azmat, Muneeza and B., Pranjal and Bai, W and Banerjee, Subhashis and Barth, Bill and Batchelder, Thomas and Batmanghelich, Kayhan and Battistella, Enzo and Beers, Andrew and Belyaev, Mikhail and Bendszus, Martin and Benson, Eze and Bernal, Jose and Bharath, Halandur Nagaraja and Biros, George and Bisdas, Sotirios and Brown, James and Cabezas, Mariano and Cao, Shilei and Cardoso, Jorge M and Carver, Eric N and Casamitjana, Adri{\`a} and Castillo, Laura Silvana and Cat{\`a}, Marcel and Cattin, Philippe and Cerigues, Albert and Chagas, Vinicius S and Chandra, Siddhartha and Chang, Yi-Ju and Chang, Shiyu and Chang, Ken and Chazalon, Joseph and Chen, Shengcong and Chen, Wei and Chen, Jefferson W and Chen, Zhaolin and Cheng, Kun and Choudhury, Ahana Roy and Chylla, Roger and Cl{\'e}rigues, Albert and Colleman, Steven and Colmeiro, Ramiro German Rodriguez and Combalia, Marc and Costa, Anthony and Cui, Xiaomeng and Dai, Zhenzhen and Dai, Lutao and Daza, Laura Alexandra and Deutsch, Eric and Ding, Changxing and Dong, Chao and Dong, Shidu and Dudzik, Wojciech and Eaton-Rosen, Zach and Egan, Gary and Escudero, Guilherme and Estienne, Th{\'e}o and Everson, Richard and Fabrizio, Jonathan and Fan, Yong and Fang, Longwei and Feng, Xue and Ferrante, Enzo and Fidon, Lucas and Fischer, Martin and French, Andrew P and Fridman, Naomi and Fu, Huan and Fuentes, David and Gao, Yaozong and Gates, Evan and Gering, David and Gholami, Amir and Gierke, Willi and Glocker, Ben and Gong, Mingming and Gonz{\'a}lez-Vill{\'a}, Sandra and Grosges, T and Guan, Yuanfang and Guo, Sheng and Gupta, Sudeep and Han, Woo-Sup and Han, Il Song and Harmuth, Konstantin and He, Huiguang and Hern{\'a}ndez-Sabat{\'e}, Aura and Herrmann, Evelyn and Himthani, Naveen and Hsu, Winston and Hsu, Cheyu and Hu, Xiaojun and Hu, Xiaobin and Hu, Yan and Hu, Yifan and Hua, Rui and Huang, Teng-Yi and Huang, Weilin and Van Huffel, Sabine and Huo, Quan and Hv, Vivek and Iftekharuddin, Khan M and Isensee, Fabian and Islam, Mobarakol and Jackson, Aaron S and Jambawalikar, Sachin R and Jesson, Andrew and Jian, Weijian and Jin, Peter and Jose, V Jeya Maria and Jungo, Alain and Kainz, B and Kamnitsas, Konstantinos and Kao, Po-Yu and Karnawat, Ayush and Kellermeier, Thomas and Kermi, Adel and Keutzer, Kurt and Khadir, Mohamed Tarek and Khened, Mahendra and Kickingereder, Philipp and Kim, Geena and King, Nik and Knapp, Haley and Knecht, Urspeter and Kohli, Lisa and Kong, Deren and Kong, Xiangmao and Koppers, Simon and Kori, Avinash and Krishnamurthi, Ganapathy and Krivov, Egor and Kumar, Piyush and Kushibar, Kaisar and Lachinov, Dmitrii and Lambrou, Tryphon and Lee, Joon and Lee, Chengen and Lee, Yuehchou and Lee, M and Lefkovits, Szidonia and Lefkovits, Laszlo and Levitt, James and Li, Tengfei and Li, Hongwei and Li, Wenqi and Li, Hongyang and Li, Xiaochuan and Li, Yuexiang and Li, Heng and Li, Zhenye and Li, Xiaoyu and Li, Zeju and Li, Xiaogang and Li, Wenqi and Lin, Zheng-Shen and Lin, Fengming and Lio, Pietro and Liu, Chang and Liu, Boqiang and Liu, Xiang and Liu, Mingyuan and Liu, Ju and Liu, Luyan and Llado, Xavier and Lopez, Marc Moreno and Lorenzo, Pablo Ribalta and Lu, Zhentai and Luo, Lin and Luo, Zhigang and Ma, Jun and Ma, Kai and Mackie, Thomas and Madabushi, Anant and Mahmoudi, Issam and Maier-Hein, Klaus H and Maji, Pradipta and Mammen, C P and Mang, Andreas and Manjunath, B S and Marcinkiewicz, Michal and McDonagh, S and McKenna, Stephen and McKinley, Richard and Mehl, Miriam and Mehta, Sachin and Mehta, Raghav and Meier, Raphael and Meinel, Christoph and Merhof, Dorit and Meyer, Craig and Miller, Robert and Mitra, Sushmita and Moiyadi, Aliasgar and Molina-Garcia, David and Monteiro, Miguel A B and Mrukwa, Grzegorz and Myronenko, Andriy and Nalepa, Jakub and Ngo, Thuyen and Nie, Dong and Ning, Holly and Niu, Chen and Nuechterlein, Nicholas K and Oermann, Eric and Oliveira, Arlindo and Oliveira, Diego D C and Oliver, Arnau and Osman, Alexander F I and Ou, Yu-Nian and Ourselin, Sebastien and Paragios, Nikos and Park, Moo Sung and Paschke, Brad and Pauloski, J Gregory and Pawar, Kamlesh and Pawlowski, Nick and Pei, Linmin and Peng, Suting and Pereira, Silvio M and Perez-Beteta, Julian and Perez-Garcia, Victor M and Pezold, Simon and Pham, Bao and Phophalia, Ashish and Piella, Gemma and Pillai, G N and Piraud, Marie and Pisov, Maxim and Popli, Anmol and Pound, Michael P and Pourreza, Reza and Prasanna, Prateek and Prkovska, Vesna and Pridmore, Tony P and Puch, Santi and Puybareau, {\'E}lodie and Qian, Buyue and Qiao, Xu and Rajchl, Martin and Rane, Swapnil and Rebsamen, Michael and Ren, Hongliang and Ren, Xuhua and Revanuru, Karthik and Rezaei, Mina and Rippel, Oliver and Rivera, Luis Carlos and Robert, Charlotte and Rosen, Bruce and Rueckert, Daniel and Safwan, Mohammed and Salem, Mostafa and Salvi, Joaquim and Sanchez, Irina and S{\'a}nchez, Irina and Santos, Heitor M and Sartor, Emmett and Schellingerhout, Dawid and Scheufele, Klaudius and Scott, Matthew R and Scussel, Artur A and Sedlar, Sara and Serrano-Rubio, Juan Pablo and Shah, N Jon and Shah, Nameetha and Shaikh, Mazhar and Shankar, B Uma and Shboul, Zeina and Shen, Haipeng and Shen, Dinggang and Shen, Linlin and Shen, Haocheng and Shenoy, Varun and Shi, Feng and Shin, Hyung Eun and Shu, Hai and Sima, Diana and Sinclair, M and Smedby, Orjan and Snyder, James M and Soltaninejad, Mohammadreza and Song, Guidong and Soni, Mehul and Stawiaski, Jean and Subramanian, Shashank and Sun, Li and Sun, Roger and Sun, Jiawei and Sun, Kay and Sun, Yu and Sun, Guoxia and Sun, Shuang and Suter, Yannick R and Szilagyi, Laszlo and Talbar, Sanjay and Tao, Dacheng and Tao, Dacheng and Teng, Zhongzhao and Thakur, Siddhesh and Thakur, Meenakshi H and Tharakan, Sameer and Tiwari, Pallavi and Tochon, Guillaume and Tran, Tuan and Tsai, Yuhsiang M and Tseng, Kuan-Lun and Tuan, Tran Anh and Turlapov, Vadim and Tustison, Nicholas and Vakalopoulou, Maria and Valverde, Sergi and Vanguri, Rami and Vasiliev, Evgeny and Ventura, Jonathan and Vera, Luis and Vercauteren, Tom and Verrastro, C A and Vidyaratne, Lasitha and Vilaplana, Veronica and Vivekanandan, Ajeet and Wang, Guotai and Wang, Qian and Wang, Chiatse J and Wang, Weichung and Wang, Duo and Wang, Ruixuan and Wang, Yuanyuan and Wang, Chunliang and Wang, Guotai and Wen, Ning and Wen, Xin and Weninger, Leon and Wick, Wolfgang and Wu, Shaocheng and Wu, Qiang and Wu, Yihong and Xia, Yong and Xu, Yanwu and Xu, Xiaowen and Xu, Peiyuan and Yang, Tsai-Ling and Yang, Xiaoping and Yang, Hao-Yu and Yang, Junlin and Yang, Haojin and Yang, Guang and Yao, Hongdou and Ye, Xujiong and Yin, Changchang and Young-Moxon, Brett and Yu, Jinhua and Yue, Xiangyu and Zhang, Songtao and Zhang, Angela and Zhang, Kun and Zhang, Xuejie and Zhang, Lichi and Zhang, Xiaoyue and Zhang, Yazhuo and Zhang, Lei and Zhang, Jianguo and Zhang, Xiang and Zhang, Tianhao and Zhao, Sicheng and Zhao, Yu and Zhao, Xiaomei and Zhao, Liang and Zheng, Yefeng and Zhong, Liming and Zhou, Chenhong and Zhou, Xiaobing and Zhou, Fan and Zhu, Hongtu and Zhu, Jin and Zhuge, Ying and Zong, Weiwei and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Davatzikos, Christos and van Leemput, Koen and Menze, Bjoern},
  abstract={Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset. t has also been a continuously evolving/growing dataset.},
  journal={arXiv preprint},
  arxiv={1811.02629},
  year={2018},
  month={November}
}

@techreport{xia2025decoupled,
  abbr={PrePrint},
  bibtex_show={true},
  title={Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models},
  author={Xia, Tian and Ribeiro, Fabio De Sousa and Rasal, Rajat R and Kori, Avinash and Mehta, Raghav and Glocker, Ben},
  abstract={Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes—a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.},
  journal={arXiv preprint},
  arxiv={2506.14399},
  year={2025},
  month={June}
}


@techreport{roschewitz2025where,
  abbr={PrePrint},
  bibtex_show={true},
  title={Where are we with calibration under dataset shift in image classification?},
  author={Roschewitz, Melanie and Mehta, Raghav and De Sousa Ribeiro, Fabio and Glocker, Ben},
  abstract={We conduct an extensive study on the state of calibration under real-world dataset shift for image classification. Our work provides important insights on the choice of post-hoc and in-training calibration techniques, and yields practical guidelines for all practitioners interested in robust calibration under shift. We compare various post-hoc calibration methods, and their interactions with common in-training calibration strategies (e.g., label smoothing), across a wide range of natural shifts, on eight different classification tasks across several imaging domains. We find that: (i) simultaneously applying entropy regularisation and label smoothing yield the best calibrated raw probabilities under dataset shift, (ii) post-hoc calibrators exposed to a small amount of semantic out-of-distribution data (unrelated to the task) are most robust under shift, (iii) recent calibration methods specifically aimed at increasing calibration under shifts do not necessarily offer significant improvements over simpler post-hoc calibration methods, (iv) improving calibration under shifts often comes at the cost of worsening in-distribution calibration. Importantly, these findings hold for randomly initialised classifiers, as well as for those finetuned from foundation models, the latter being consistently better calibrated compared to models trained from scratch. Finally, we conduct an in-depth analysis of ensembling effects, finding that (i) applying calibration prior to ensembling (instead of after) is more effective for calibration under shifts, (ii) for ensembles, OOD exposure deteriorates the ID-shifted calibration trade-off, (iii) ensembling remains one of the most effective methods to improve calibration robustness and, combined with finetuning from foundation models, yields best calibration results overall.},
  journal={arXiv preprint},
  arxiv={2507.07780},
  year={2025},
  month={July}
}


@phdthesis{mcgillthesis,
  abbr={Thesis},
  bibtex_show={true},
  title={Integrating Bayesian Deep Learning Uncertainties in Medical Image Analysis},
  abstract={Although Deep Learning (DL) models have been shown to perform very well on various medical imaging tasks, inference in the presence of pathology presents several challenges to common models. These challenges impede the integration of DL models into real clinical workflows. Deployment of these models into real clinical contexts requires: (1) that the confidence in DL model predictions be accurately expressed in the form of uncertainties and (2) that they exhibit robustness and fairness across different sub-populations. Quantifying the reliability of DL model predictions in the form of uncertainties could enable clinical review of the most uncertain regions, thereby building trust and paving the way toward clinical translation. Similarly, by embedding uncertainty estimates across cascaded inference tasks, prevalent in medical image analysis, performance on the downstream inference tasks should also be improved. In this thesis, we develop an uncertainty quantification score for the task of Brain Tumour Segmentation. We evaluate the score's usefulness during the two consecutive Brain Tumour Segmentation (BraTS) challenges, BraTS 2019 and BraTS 2020. Overall, our findings confirm the importance and complementary value that uncertainty estimates provide to segmentation algorithms, highlighting the need for uncertainty quantification in medical image analyses. We further show the importance of uncertainty estimates in medical image analysis by propagating uncertainty generated by upstream tasks into the downstream task of interest. Our results on three different clinically relevant tasks indicate that uncertainty propagation helps improve the performance of the downstream task of interest. Additionally, we combine the aspect of uncertainty estimates with fairness across demographic subgroups into the picture. By performing extensive experiments on multiple tasks, we show that popular ML methods for achieving fairness across different subgroups, such as data-balancing and distributionally robust optimization, succeed in terms of the model performances for some of the tasks. However, this can come at the cost of poor uncertainty estimates associated with the model predictions. This tradeoff must be mitigated if fairness models are to be adopted in medical image analysis. In the last part of the thesis, we look at Active Learning (AL) for reduced manual labeling of a dataset. Specifically, we present an information-theoretic active learning framework that guides the optimal selection of images for labeling. Results indicate that the proposed framework outperforms several existing AL methods, and by careful design choices, it can be integrated into existing deep learning classifiers with minimal computational overhead},
  author={Mehta, Raghav},
  year={2023},
  month={December},
  address={Montreal, Canada},
  school={McGill University},
  type={PhD thesis},
  html={https://escholarship.mcgill.ca/concern/theses/9p290g949},
  slides={Raghav_Mehta_PhD_Defense_presentation.pdf}
}

@mastersthesis{iiitthesis,
  abbr={Thesis},
  bibtex_show={true},
  title={Population specific template construction and brain structure segmentation using deep learning methods},
  abstract={A brain template, such as MNI152 is a digital (magentic resonance image or MRI) representation of the brain in a reference coordinate system for the neuroscience research. Structural atlases, such as AAL and DKA, delineate the brain into cortical and subcortical structures which are used in Voxel Based Morphometry (VBM) and fMRI analysis. Many population specific templates, i.e. Chinese, Korean, etc., have been constructed recently. It was observed that there are morphological differences between the average brain of the eastern and the western population. In this thesis, we report on the development of a population specific brain template for the young Indian population. This is derived from a multi-centeric MRI dataset of 100 Indian adults (21 - 30 years old). Measurements made with this template indicated that the Indian brain, on average, is smaller in height and width compared to the Caucasian and the Chinese brain. A second problem this thesis examines is automated segmentation of cortical and non-cortical human brain structures, using multiple structural atlases. This has been hitherto approached using computationally expensive non-rigid registration followed by label fusion. We propose an alternative approach for this using a Convolutional Neural Network (CNN) which classifies a voxel into one of many structures. Evaluation of the proposed method on various datasets showed that the mean Dice coefficient varied from 0.844±0.031 to 0.743±0.019 for datasets with the least (32) and the most (134) number of labels, respectively. These figures are marginally better or on par with those obtained with the current state of the art methods on nearly all datasets, at a reduced computational time. We also propose an end-to-end trainable Fully Convolutional Neural Network (FCNN) architecture called the M-net, for segmenting deep (human) brain structures. A novel scheme is used to learn to combine and represent 3D context information of a given slice in a 2D slice. Consequently, the M-net utilizes only 2D convolution though it operates on 3D data. Experiment results show that the M-net outperforms other state-of-the-art model-based segmentation methods in terms of dice coefficient and is at least 3 times faster than them.},
  author={Mehta, Raghav},
  year={2017},
  month={July},
  address={Hyderabad, India},
  school={International Institute of Information Technology -  Hyderabad (IIIT-H)},
  type={MS thesis},
  html={https://web2py.iiit.ac.in/research_centres/publications/view_publication/mastersthesis/511},
  slides={MS_thesis_presentation.pdf}
}
